{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Introduction and Imports\n",
    "\n",
    "# Deepfake Detection Evaluation Notebook\n",
    "# Using calibrated thresholds from ASVspoof2021 DF SSL_Anti-spoofing model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import functions from our eval_metrics.py\n",
    "sys.path.append('.')\n",
    "from eval_metrics import compute_metrics\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using calibrated thresholds from ASVspoof2021 DF SSL_Anti-spoofing model:\n",
      "EER threshold: -3.5324\n",
      "minDCF threshold: -1.4866\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define the calibrated thresholds from previous analysis\n",
    "\n",
    "CALIBRATED_THRESHOLDS = {\n",
    "    'eer_threshold': -3.5324,  \n",
    "    'min_dcf_threshold': -1.4866\n",
    "}\n",
    "\n",
    "print(\"Using calibrated thresholds from ASVspoof2021 DF SSL_Anti-spoofing model:\")\n",
    "print(f\"EER threshold: {CALIBRATED_THRESHOLDS['eer_threshold']:.4f}\")\n",
    "print(f\"minDCF threshold: {CALIBRATED_THRESHOLDS['min_dcf_threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scores from ../files/sls_scores.pkl...\n",
      "Assigning 'bonafide' label to all files...\n",
      "Loaded 512 scores.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>score</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>detected_as_fake_eer</th>\n",
       "      <th>detected_as_fake_dcf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/audio_files/20250102225324-1735868804.68...</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data/audio_files/20250104163153-1736019058.11...</td>\n",
       "      <td>-5.350148</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data/audio_files/20241219201129-1734649648.19...</td>\n",
       "      <td>-0.060007</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data/audio_files/20241231190939-1735682910.29...</td>\n",
       "      <td>-14.297694</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data/audio_files/20241222180522-1734901413.22...</td>\n",
       "      <td>-11.873585</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_id      score  \\\n",
       "0  /data/audio_files/20250102225324-1735868804.68...  -0.000184   \n",
       "1  /data/audio_files/20250104163153-1736019058.11...  -5.350148   \n",
       "2  /data/audio_files/20241219201129-1734649648.19...  -0.060007   \n",
       "3  /data/audio_files/20241231190939-1735682910.29... -14.297694   \n",
       "4  /data/audio_files/20241222180522-1734901413.22... -11.873585   \n",
       "\n",
       "   expected_label  detected_as_fake_eer  detected_as_fake_dcf  \n",
       "0               1                 False                 False  \n",
       "1               1                  True                  True  \n",
       "2               1                 False                 False  \n",
       "3               1                  True                  True  \n",
       "4               1                  True                  True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 512 files\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Scores from File\n",
    "\n",
    "def load_scores_from_file(scores_file, labels_file=None, label_type=None):\n",
    "    \"\"\"\n",
    "    Load detection scores from pickle file and optionally load or assign labels.\n",
    "    \n",
    "    Args:\n",
    "        scores_file: Path to pickle file containing scores\n",
    "        labels_file: Optional path to file containing labels\n",
    "        label_type: If no labels_file, assign this label to all files ('bonafide' or 'spoof')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with file IDs, scores, and labels\n",
    "    \"\"\"\n",
    "    print(f\"Loading scores from {scores_file}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load scores from pickle file\n",
    "        with open(scores_file, 'rb') as f:\n",
    "            scores_data = pickle.load(f)\n",
    "        \n",
    "        # Convert the loaded data to a DataFrame\n",
    "        # This assumes the pickle file contains either:\n",
    "        # 1. A dictionary with file IDs as keys and scores as values\n",
    "        # 2. A pandas DataFrame with at least file ID and score columns\n",
    "        # 3. A list of tuples/lists with (file_id, score)\n",
    "        \n",
    "        if isinstance(scores_data, dict):\n",
    "            results_df = pd.DataFrame({\n",
    "                'file_id': list(scores_data.keys()),\n",
    "                'score': list(scores_data.values())\n",
    "            })\n",
    "        elif isinstance(scores_data, pd.DataFrame):\n",
    "            # Ensure the DataFrame has required columns\n",
    "            required_cols = ['file_id', 'score']\n",
    "            if not all(col in scores_data.columns for col in required_cols):\n",
    "                # Try to find suitable columns or rename existing ones\n",
    "                if 'filename' in scores_data.columns:\n",
    "                    scores_data['file_id'] = scores_data['filename']\n",
    "                if 'prediction' in scores_data.columns:\n",
    "                    scores_data['score'] = scores_data['prediction']\n",
    "                \n",
    "            results_df = scores_data[['file_id', 'score']].copy()\n",
    "        elif isinstance(scores_data, list):\n",
    "            # Assuming list of tuples/lists with (file_id, score)\n",
    "            results_df = pd.DataFrame(scores_data, columns=['file_id', 'score'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scores data format: {type(scores_data)}\")\n",
    "        \n",
    "        # Handle labels\n",
    "        if labels_file:\n",
    "            print(f\"Loading labels from {labels_file}...\")\n",
    "            # Load labels from file (assume it's a pickled dict or DataFrame)\n",
    "            with open(labels_file, 'rb') as f:\n",
    "                labels_data = pickle.load(f)\n",
    "            \n",
    "            if isinstance(labels_data, dict):\n",
    "                # Convert dict to Series for easier merging\n",
    "                labels_series = pd.Series(labels_data, name='expected_label')\n",
    "                labels_df = pd.DataFrame({'file_id': labels_series.index, 'expected_label': labels_series.values})\n",
    "                \n",
    "                # Merge with scores DataFrame\n",
    "                results_df = pd.merge(results_df, labels_df, on='file_id', how='left')\n",
    "            elif isinstance(labels_data, pd.DataFrame):\n",
    "                # Assuming DataFrame has file_id and label columns\n",
    "                if 'file_id' in labels_data.columns and 'label' in labels_data.columns:\n",
    "                    results_df = pd.merge(results_df, \n",
    "                                          labels_data[['file_id', 'label']], \n",
    "                                          on='file_id', how='left')\n",
    "                    results_df['expected_label'] = results_df['label']\n",
    "                    results_df.drop('label', axis=1, inplace=True, errors='ignore')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported labels data format: {type(labels_data)}\")\n",
    "        elif label_type:\n",
    "            print(f\"Assigning '{label_type}' label to all files...\")\n",
    "            # Assign the same label to all files\n",
    "            if label_type.lower() in ['bonafide', 'genuine', 'real', '1']:\n",
    "                results_df['expected_label'] = 1  # Bonafide\n",
    "            elif label_type.lower() in ['spoof', 'fake', 'deepfake', '0']:\n",
    "                results_df['expected_label'] = 0  # Spoof\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported label type: {label_type}. Use 'bonafide' or 'spoof'\")\n",
    "        else:\n",
    "            # No labels provided\n",
    "            print(\"No labels provided. Analysis will be limited.\")\n",
    "            results_df['expected_label'] = np.nan\n",
    "        \n",
    "        # Compute detection results based on thresholds\n",
    "        results_df['detected_as_fake_eer'] = results_df['score'] < CALIBRATED_THRESHOLDS['eer_threshold']\n",
    "        results_df['detected_as_fake_dcf'] = results_df['score'] < CALIBRATED_THRESHOLDS['min_dcf_threshold']\n",
    "        \n",
    "        print(f\"Loaded {len(results_df)} scores.\")\n",
    "        return results_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scores: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load scores from the pickle file\n",
    "# You can either:\n",
    "# 1. Just load scores and assign all as spoof: label_type='spoof'\n",
    "# 2. Load scores and a separate labels file: labels_file='path/to/labels.pkl'\n",
    "# 3. Just load scores without labels (limited analysis): labels_file=None, label_type=None\n",
    "\n",
    "SCORES_FILE = \"../files/sls_scores.pkl\"\n",
    "# LABELS_FILE = None  # Optional path to labels file\n",
    "LABEL_TYPE = \"bonafide\"  # Or \"bonafide\", or None if using LABELS_FILE\n",
    "\n",
    "results_df = load_scores_from_file(SCORES_FILE, label_type=LABEL_TYPE)\n",
    "\n",
    "# Display the first few results\n",
    "if not results_df.empty:\n",
    "    display(results_df.head())\n",
    "    print(f\"Processed {len(results_df)} files\")\n",
    "else:\n",
    "    print(\"No results to display. Please check the scores file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance using EER threshold:\n",
      "threshold_used: -3.5324\n",
      "threshold_type: eer_threshold\n",
      "total_files: 512\n",
      "total_spoofed: 0\n",
      "detected_spoofed: 0\n",
      "detection_rate: None\n",
      "total_bonafide: 512\n",
      "accepted_bonafide: 201\n",
      "bonafide_acceptance_rate: 0.3926\n",
      "\n",
      "Performance using minDCF threshold:\n",
      "threshold_used: -1.4866\n",
      "threshold_type: min_dcf_threshold\n",
      "total_files: 512\n",
      "total_spoofed: 0\n",
      "detected_spoofed: 0\n",
      "detection_rate: None\n",
      "total_bonafide: 512\n",
      "accepted_bonafide: 166\n",
      "bonafide_acceptance_rate: 0.3242\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Analyze detection performance\n",
    "\n",
    "def analyze_detection_performance(results_df, threshold_key='eer_threshold'):\n",
    "    \"\"\"\n",
    "    Analyze the detection performance on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with results\n",
    "        threshold_key: Which threshold to use ('eer_threshold' or 'min_dcf_threshold')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    # Which detection column to use\n",
    "    detection_col = 'detected_as_fake_eer' if threshold_key == 'eer_threshold' else 'detected_as_fake_dcf'\n",
    "    \n",
    "    # Prepare metrics dictionary\n",
    "    metrics = {\n",
    "        'threshold_used': CALIBRATED_THRESHOLDS[threshold_key],\n",
    "        'threshold_type': threshold_key,\n",
    "        'total_files': len(results_df)\n",
    "    }\n",
    "    \n",
    "    # Check if we have expected labels\n",
    "    if 'expected_label' in results_df.columns and not results_df['expected_label'].isna().all():\n",
    "        # Calculate detection rate (how many expected fakes were detected as fake)\n",
    "        total_spoofed = len(results_df[results_df['expected_label'] == 0])\n",
    "        detected_spoofed = len(results_df[(results_df['expected_label'] == 0) & \n",
    "                                          (results_df[detection_col] == True)])\n",
    "        \n",
    "        # Calculate bonafide acceptance rate (if any bonafide samples)\n",
    "        total_bonafide = len(results_df[results_df['expected_label'] == 1])\n",
    "        accepted_bonafide = len(results_df[(results_df['expected_label'] == 1) & \n",
    "                                            (results_df[detection_col] == False)])\n",
    "        \n",
    "        # Add these metrics to the results\n",
    "        metrics.update({\n",
    "            'total_spoofed': total_spoofed,\n",
    "            'detected_spoofed': detected_spoofed,\n",
    "            'detection_rate': detected_spoofed / total_spoofed if total_spoofed > 0 else None,\n",
    "            'total_bonafide': total_bonafide,\n",
    "            'accepted_bonafide': accepted_bonafide,\n",
    "            'bonafide_acceptance_rate': accepted_bonafide / total_bonafide if total_bonafide > 0 else None\n",
    "        })\n",
    "    else:\n",
    "        # If no labels, just report detection statistics\n",
    "        detected_as_fake = results_df[detection_col].sum()\n",
    "        metrics.update({\n",
    "            'detected_as_fake': detected_as_fake,\n",
    "            'detected_as_fake_percentage': (detected_as_fake / len(results_df)) * 100,\n",
    "            'note': \"No expected labels provided. Detection statistics only.\"\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Analyze performance using both thresholds\n",
    "if not results_df.empty:\n",
    "    eer_metrics = analyze_detection_performance(results_df, 'eer_threshold')\n",
    "    dcf_metrics = analyze_detection_performance(results_df, 'min_dcf_threshold')\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nPerformance using EER threshold:\")\n",
    "    for key, value in eer_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\nPerformance using minDCF threshold:\")\n",
    "    for key, value in dcf_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
