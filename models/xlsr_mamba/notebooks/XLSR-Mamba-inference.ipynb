{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm  # Use notebook version of tqdm\n",
    "\n",
    "# Ensure we are in the correct directory within the container\n",
    "# The Dockerfile clones the repo into /app\n",
    "os.chdir('/app')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if model.py exists (it should)\n",
    "if not os.path.exists('model.py'):\n",
    "    print(\"ERROR: model.py not found in the current directory /app.\")\n",
    "    print(\"Make sure the XLSR-Mamba repository was cloned correctly in the Dockerfile.\")\n",
    "else:\n",
    "    from model import Model  # Make sure this is Model, not XLSR_Mamba\n",
    "    from utils import reproducibility\n",
    "\n",
    "# === Configuration ===\n",
    "# Adapted from the guide and repository defaults\n",
    "class Args:\n",
    "    # Model specific\n",
    "    emb_size = 144\n",
    "    num_encoders = 12\n",
    "    FT_W2V = True # Use pretrained XLSR weights\n",
    "    algo = 3  # 3 = DF (Deepfake Detection) based on checkpoint name\n",
    "\n",
    "    # Paths within the container\n",
    "    # Base XLSR model downloaded in Dockerfile and placed in /app\n",
    "    model_path = '/app/xlsr2_300m.pt'\n",
    "    # Fine-tuned checkpoints (mounted via volume)\n",
    "    # Corrected path for DF model\n",
    "    model_path_finetune = '/app/models_mounted/Bmamba3_LA_WCE_1e-06_ES144_NE12/best'\n",
    "\n",
    "    # Checkpoint averaging\n",
    "    n_average_model = 5 # Average top 5 checkpoints\n",
    "\n",
    "    # Other settings\n",
    "    loss = 'WCE' # From checkpoint path name\n",
    "    lr = 1e-6    # From checkpoint path name\n",
    "    seed = 1234\n",
    "    comment = None # Not used for inference\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "print(f\"Using device: {args.device}\")\n",
    "print(f\"Algorithm Track (algo): {args.algo}\")\n",
    "print(f\"Base XLSR Model Path: {args.model_path}\")\n",
    "print(f\"Fine-tuned Checkpoint Dir: {args.model_path_finetune}\")\n",
    "print(f\"Averaging {args.n_average_model} checkpoints.\")\n",
    "\n",
    "# === Ensure reproducibility ===\n",
    "# Check if function exists before calling\n",
    "if 'reproducibility' in globals():\n",
    "    reproducibility(args.seed, args)\n",
    "    # Assuming reproducibility function prints its own messages like:\n",
    "    # cudnn_deterministic set to False\n",
    "    # cudnn_benchmark set to True\n",
    "else:\n",
    "    print(\"Skipping reproducibility step as function 'reproducibility' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Initialize model ===\n",
    "# Ensure Model class was imported successfully\n",
    "if 'Model' in globals():\n",
    "    print(\"Initializing model...\")\n",
    "    # Use the configuration 'args' object defined in Cell 1\n",
    "    # The Model class __init__ takes 'args' and 'device'\n",
    "    model = Model(args, device=args.device).to(args.device)\n",
    "    print(\"Model initialized.\")\n",
    "\n",
    "    # === Load and average fine-tuned checkpoints ===\n",
    "    print(f\"üîÑ Averaging top-{args.n_average_model} checkpoints from {args.model_path_finetune}...\")\n",
    "\n",
    "    # Check if the directory exists (mounted volume)\n",
    "    if not os.path.isdir(args.model_path_finetune):\n",
    "         print(f\"ERROR: Checkpoint directory not found: {args.model_path_finetune}\")\n",
    "         print(\"Please ensure your local checkpoint directory is mounted correctly to /app/pretrained_models/models\")\n",
    "    else:\n",
    "        sd_avg = None\n",
    "        loaded_count = 0\n",
    "        for i in range(args.n_average_model):\n",
    "            ckpt_path = os.path.join(args.model_path_finetune, f'best_{i}.pth')\n",
    "            if not os.path.exists(ckpt_path):\n",
    "                print(f\"Warning: Checkpoint {ckpt_path} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\" -> Loading {ckpt_path}\")\n",
    "            try:\n",
    "                sd = torch.load(ckpt_path, map_location=args.device)\n",
    "                if sd_avg is None:\n",
    "                    sd_avg = sd\n",
    "                else:\n",
    "                    # Check if keys match before adding\n",
    "                    if sd_avg.keys() != sd.keys():\n",
    "                        print(f\"ERROR: Checkpoint {i} has different keys than the first one. Cannot average.\")\n",
    "                        sd_avg = None # Invalidate averaging\n",
    "                        break\n",
    "                    for key in sd:\n",
    "                        # Ensure keys exist in both dictionaries before adding\n",
    "                        if key in sd_avg:\n",
    "                           sd_avg[key] += sd[key]\n",
    "                        else:\n",
    "                           print(f\"ERROR: Key '{key}' from checkpoint {i} not found in the first checkpoint. Cannot average.\")\n",
    "                           sd_avg = None # Invalidate averaging\n",
    "                           break\n",
    "                if sd_avg is None: # Break outer loop if averaging invalidated\n",
    "                    break\n",
    "                loaded_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint {ckpt_path}: {e}\")\n",
    "                # Optionally break if one checkpoint fails, or just continue\n",
    "                # break\n",
    "\n",
    "        if sd_avg is not None and loaded_count > 0:\n",
    "            print(f\"Averaging weights over {loaded_count} loaded checkpoints...\")\n",
    "            for key in sd_avg:\n",
    "                # Ensure division is done correctly even if some checkpoints were skipped\n",
    "                sd_avg[key] = torch.true_divide(sd_avg[key], loaded_count)\n",
    "\n",
    "            print(\"Loading averaged state dict into model...\")\n",
    "            try:\n",
    "                model.load_state_dict(sd_avg)\n",
    "                model.eval()\n",
    "                print(\"‚úÖ Model loaded and averaged successfully.\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error loading averaged state_dict: {e}\")\n",
    "                print(\"This might indicate a mismatch between the averaged weights and the model architecture.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred loading the averaged state_dict: {e}\")\n",
    "\n",
    "        elif loaded_count == 0:\n",
    "             print(\"ERROR: No checkpoints were found or loaded. Model weights are not averaged.\")\n",
    "        else: # This happens if averaging failed mid-way\n",
    "             print(\"ERROR: Could not average checkpoints due to errors (e.g., key mismatch, load error).\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model class not imported. Cannot initialize model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Preparation (with Optional Multiprocessing Check)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool, cpu_count # Import multiprocessing components\n",
    "\n",
    "# --- Configuration ---\n",
    "data_dir = '/app/data/brspeech_df/'\n",
    "target_file_extension = 'flac'\n",
    "target_sample_rate = 16000 # Should match model's expected sample rate\n",
    "num_check_workers = max(1, cpu_count() // 2) # Use half the CPU cores for checking, minimum 1\n",
    "skip_file_checking = True # <<< Set to True to skip the detailed file check >>>\n",
    "\n",
    "print(f\"üîç Searching for .{target_file_extension} files in {data_dir}...\")\n",
    "glob_pattern = os.path.join(data_dir, f'**/*.{target_file_extension}')\n",
    "# Initial list of all found files\n",
    "initial_audio_files = glob.glob(glob_pattern, recursive=True)\n",
    "\n",
    "# --- Define Worker Function for File Check ---\n",
    "# This function needs to be defined at the top level for multiprocessing\n",
    "def check_audio_file(f_path):\n",
    "    \"\"\"Checks if an audio file is accessible, non-empty, and returns its path if valid.\"\"\"\n",
    "    try:\n",
    "        info = torchaudio.info(f_path)\n",
    "        if info.num_frames > 0:\n",
    "            return f_path # Return path if valid and non-empty\n",
    "        else:\n",
    "            # Optionally print skipped empty files here, but it might get noisy with many workers\n",
    "            # print(f\"Skipping empty file: {f_path}\")\n",
    "            return None # Skip empty files\n",
    "    except Exception as e:\n",
    "        # Optionally print skipped error files here\n",
    "        # print(f\"Skipping file due to error: {f_path} ({e})\")\n",
    "        return None # Skip files that cause errors\n",
    "\n",
    "# --- Perform File Checking (Optional) ---\n",
    "if not initial_audio_files:\n",
    "    print(f\"‚ö†Ô∏è No .{target_file_extension} files found using pattern '{glob_pattern}'.\")\n",
    "    audio_files = [] # Ensure audio_files is defined for later checks\n",
    "elif skip_file_checking:\n",
    "    audio_files = initial_audio_files\n",
    "    print(f\"‚úÖ Skipping detailed file check. Proceeding with all {len(audio_files)} found files.\")\n",
    "else:\n",
    "    print(f\"Found {len(initial_audio_files)} potential .{target_file_extension} files.\")\n",
    "    print(f\"Verifying access and checking for non-empty files using {num_check_workers} workers...\")\n",
    "\n",
    "    valid_audio_files = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Use multiprocessing Pool\n",
    "    with Pool(processes=num_check_workers) as pool:\n",
    "        # Use imap_unordered for potentially better performance and memory usage\n",
    "        # tqdm shows progress as results come in\n",
    "        results_iterator = pool.imap_unordered(check_audio_file, initial_audio_files)\n",
    "        for result_path in tqdm(results_iterator, total=len(initial_audio_files), desc=\"Checking files\"):\n",
    "            if result_path:\n",
    "                valid_audio_files.append(result_path)\n",
    "            else:\n",
    "                skipped_count += 1 # Count files that returned None (error or empty)\n",
    "\n",
    "    audio_files = valid_audio_files\n",
    "    print(f\"\\nProceeding with {len(audio_files)} accessible, non-empty files ({skipped_count} skipped due to errors or being empty).\")\n",
    "\n",
    "\n",
    "# --- Define Custom Dataset ---\n",
    "class AudioInferenceDataset(Dataset):\n",
    "    def __init__(self, file_paths, target_sr):\n",
    "        self.file_paths = file_paths # Assumes this list is pre-filtered (or checking was skipped)\n",
    "        self.target_sr = target_sr\n",
    "        self.resamplers = {} # Cache resamplers to avoid re-creation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.file_paths[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            # Resample if necessary\n",
    "            if sample_rate != self.target_sr:\n",
    "                # Reuse or create resampler for the specific original sample rate\n",
    "                if sample_rate not in self.resamplers:\n",
    "                    self.resamplers[sample_rate] = T.Resample(orig_freq=sample_rate, new_freq=self.target_sr)\n",
    "                resampler = self.resamplers[sample_rate]\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Ensure mono (select first channel if stereo)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform[0, :].unsqueeze(0) # Result shape: [1, num_samples]\n",
    "\n",
    "            # Ensure float32 (required by most models)\n",
    "            waveform = waveform.float()\n",
    "\n",
    "            # Remove channel dimension - model likely expects [num_samples]\n",
    "            # or [batch, num_samples] after padding in the collate function.\n",
    "            waveform = waveform.squeeze(0) # Result shape: [num_samples]\n",
    "\n",
    "            # Return the processed waveform tensor and the original file path\n",
    "            return waveform, audio_path\n",
    "\n",
    "        except Exception as e:\n",
    "            # This might catch errors not caught during pre-filtering (e.g., corrupted data)\n",
    "            # Crucially important if skip_file_checking is True\n",
    "            print(f\"Error loading/processing {audio_path} within dataset __getitem__: {e}\")\n",
    "            # Return None for the waveform to indicate an error for this specific file\n",
    "            return None, audio_path\n",
    "\n",
    "# --- Instantiate Dataset ---\n",
    "# Only create the dataset if valid files were found (or checking was skipped)\n",
    "if audio_files:\n",
    "    inference_dataset = AudioInferenceDataset(audio_files, target_sample_rate)\n",
    "    print(f\"‚úÖ Dataset created with {len(inference_dataset)} items.\")\n",
    "else:\n",
    "    inference_dataset = None # Set to None if no files were found initially\n",
    "    print(\"Dataset not created as no audio files were found.\")\n",
    "\n",
    "# Note: The DataLoader will be created in the next cell (Cell 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Batched Inference (Modified for OOM/Triton Errors)\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence # For handling variable length sequences in a batch\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import traceback # For detailed error printing\n",
    "import gc # For garbage collection\n",
    "\n",
    "# --- Configuration ---\n",
    "batch_size = 1  # <<< Set batch size to 1 to avoid padding OOM errors >>>\n",
    "num_workers = 2  # Number of parallel workers for data loading (adjust based on your CPU cores)\n",
    "\n",
    "# --- Check if dataset and model are ready ---\n",
    "if 'inference_dataset' not in globals() or inference_dataset is None or len(inference_dataset) == 0:\n",
    "    print(\"üö´ Dataset not found or is empty. Please run Cell 3 successfully first.\")\n",
    "elif 'model' not in globals() or not hasattr(model, 'eval'):\n",
    "     print(\"üö´ Model 'model' not found or not initialized. Please run Cell 2 first.\")\n",
    "else:\n",
    "    # --- Collate Function (Simplified for batch_size=1, but keep padding for consistency) ---\n",
    "    # Kept the padding logic, but with batch_size=1, it won't actually pad.\n",
    "    def pad_collate_fn(batch):\n",
    "        valid_items = [(wf, path) for wf, path in batch if wf is not None]\n",
    "        if not valid_items:\n",
    "            return None, None, None\n",
    "        waveforms, paths = zip(*valid_items)\n",
    "        lengths = torch.tensor([wf.size(0) for wf in waveforms], dtype=torch.long)\n",
    "        padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)\n",
    "        return padded_waveforms, lengths, list(paths)\n",
    "\n",
    "    # --- Create DataLoader ---\n",
    "    device = args.device # Get device from Cell 1 args\n",
    "    inference_dataloader = DataLoader(\n",
    "        inference_dataset,\n",
    "        batch_size=batch_size, # Using batch_size = 1\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device == 'cuda' else False,\n",
    "        collate_fn=pad_collate_fn\n",
    "    )\n",
    "    print(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "    # --- Inference Loop ---\n",
    "    results = []\n",
    "    print(f\"üöÄ Starting inference (batch size 1) on {device}...\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    for padded_waveforms_batch, lengths_batch, paths_batch in tqdm(inference_dataloader, desc=\"Inferencing Files\"):\n",
    "\n",
    "        if padded_waveforms_batch is None:\n",
    "            # This could happen if the single item in the batch failed loading in __getitem__\n",
    "            print(f\"Skipping batch for {paths_batch[0] if paths_batch else 'unknown file'} due to loading error.\")\n",
    "            if paths_batch: # Ensure paths_batch is not empty\n",
    "                 results.append({'filename': os.path.relpath(paths_batch[0], data_dir), 'score': float('nan')})\n",
    "            continue\n",
    "\n",
    "        padded_waveforms_batch = padded_waveforms_batch.to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(padded_waveforms_batch)\n",
    "\n",
    "                # --- Process Output Scores (Adjust based on model output) ---\n",
    "                # Assuming output is [1, 1] or [1, num_classes] for batch_size=1\n",
    "                # Option 1: Output is [1, 1]\n",
    "                scores_batch = outputs.squeeze().cpu().tolist() # Squeeze removes dims of size 1\n",
    "                # Option 2: Output is [1, num_classes] (e.g., [bona_fide, spoof])\n",
    "                # score_index = 1\n",
    "                # scores_batch = outputs[0, score_index].cpu().item() # Get single item score\n",
    "\n",
    "                # Ensure scores_batch is a list, even if squeeze resulted in a float\n",
    "                if not isinstance(scores_batch, list):\n",
    "                     scores_batch = [scores_batch] # Make it a list containing the single score\n",
    "\n",
    "            # Since batch_size is 1, lengths should match\n",
    "            for path, score in zip(paths_batch, scores_batch):\n",
    "                results.append({\n",
    "                    'filename': os.path.relpath(path, data_dir),\n",
    "                    'score': score\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error processing file: {paths_batch[0]}...\")\n",
    "            print(f\"Error type: {type(e).__name__}, Message: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full traceback if needed\n",
    "\n",
    "            # Add error indicator for the failed file\n",
    "            results.append({\n",
    "                'filename': os.path.relpath(paths_batch[0], data_dir),\n",
    "                'score': float('nan') # Indicate error with NaN\n",
    "            })\n",
    "\n",
    "            # --- Attempt to clear CUDA cache after error ---\n",
    "            if device == 'cuda':\n",
    "                print(\"Attempting to clear CUDA cache...\")\n",
    "                del padded_waveforms_batch # Explicitly delete tensor\n",
    "                if 'outputs' in locals(): # Check if outputs was assigned before error\n",
    "                    del outputs\n",
    "                gc.collect() # Run Python garbage collector\n",
    "                torch.cuda.empty_cache() # Ask PyTorch to release cached memory\n",
    "                print(\"CUDA cache cleared.\")\n",
    "\n",
    "        finally:\n",
    "             # Explicitly delete tensors from this iteration to help memory management, even on success\n",
    "             if 'padded_waveforms_batch' in locals():\n",
    "                 del padded_waveforms_batch\n",
    "             if 'outputs' in locals():\n",
    "                 del outputs\n",
    "\n",
    "    # --- Display and Save Results ---\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\n--- Inference Results ---\")\n",
    "        print(\"Score Summary:\")\n",
    "        print(results_df['score'].describe())\n",
    "        error_count = results_df['score'].isna().sum()\n",
    "        total_files = len(results_df)\n",
    "        success_count = total_files - error_count\n",
    "        print(f\"\\nProcessed {total_files} files: {success_count} successful, {error_count} errors (marked as NaN).\")\n",
    "        print(\"\\nFirst 5 results:\")\n",
    "        print(results_df.head())\n",
    "        print(\"\\nLast 5 results:\")\n",
    "        print(results_df.tail())\n",
    "\n",
    "        # --- Optional: Save to CSV ---\n",
    "        output_dir = '/app/outputs'\n",
    "        output_csv_path = os.path.join(output_dir, 'xlsr_mamba_inference_scores_bs1.csv') # Changed filename\n",
    "        print(f\"\\nüíæ Saving results to {output_csv_path}...\")\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            results_df.to_csv(output_csv_path, index=False, float_format='%.8f')\n",
    "            print(\"üíæ Results saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results to CSV: {e}\")\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
