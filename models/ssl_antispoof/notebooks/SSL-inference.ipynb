{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39507a4f",
   "metadata": {},
   "source": [
    "# ASVspoof DF Evaluation Notebook\n",
    "\n",
    "This notebook performs inference on your own list of WAV files using a pretrained DF detection model.  \n",
    "The file `yourtts_wav_files.txt` contains the full paths to your audio files.  \n",
    "We import the model and necessary functions from the original repository without modifying any files.\n",
    "\n",
    "**Steps:**\n",
    "1. Import dependencies and define utility functions.\n",
    "2. Define a custom dataset to load your WAV files.\n",
    "3. Load your file list.\n",
    "4. Create a DataLoader.\n",
    "5. Load the pretrained model and set it to evaluation mode.\n",
    "6. Run inference and collect scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Cell: Generate WAV file list from directory\n",
    "import os\n",
    "\n",
    "def get_audio_files_recursive(\n",
    "    folder_path,\n",
    "    formats=('wav', 'flac'),\n",
    "    exclude_prefix=('.', '__'),\n",
    "    required_path_segment=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively find audio files in a directory with specified extensions,\n",
    "    optionally requiring a specific segment in their path.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Root directory to search.\n",
    "        formats (tuple): File extensions to include (lowercase).\n",
    "        exclude_prefix (tuple): Skip directories starting with these prefixes.\n",
    "        required_path_segment (str, optional): \n",
    "            A string that must be part of the file's path.\n",
    "            Example: '/test/', os.sep + 'test' + os.sep, or 'test_file'.\n",
    "    \n",
    "    Returns:\n",
    "        list: Full paths to audio files.\n",
    "    \"\"\"\n",
    "    valid_files = []\n",
    "    normalized_segment_for_dir_check = None\n",
    "\n",
    "    if required_path_segment:\n",
    "        # Heuristic: if it looks like a path segment for a directory\n",
    "        is_dir_segment = (\n",
    "            required_path_segment.startswith(os.sep) or\n",
    "            required_path_segment.endswith(os.sep) or\n",
    "            os.sep in required_path_segment\n",
    "        )\n",
    "        if is_dir_segment:\n",
    "            normalized_segment_for_dir_check = os.path.normpath(\n",
    "                required_path_segment\n",
    "            )\n",
    "            if not normalized_segment_for_dir_check.startswith(os.sep):\n",
    "                normalized_segment_for_dir_check = (\n",
    "                    os.sep + normalized_segment_for_dir_check\n",
    "                )\n",
    "            if not normalized_segment_for_dir_check.endswith(os.sep):\n",
    "                normalized_segment_for_dir_check = (\n",
    "                    normalized_segment_for_dir_check + os.sep\n",
    "                )\n",
    "        else: # Treat as a general substring\n",
    "            normalized_segment_for_dir_check = required_path_segment\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Skip hidden/special directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(exclude_prefix)]\n",
    "        \n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1][1:].lower() # Get ext without dot\n",
    "            if ext in formats:\n",
    "                full_path = os.path.join(root, file)\n",
    "                normalized_full_path = os.path.normpath(full_path)\n",
    "\n",
    "                if normalized_segment_for_dir_check:\n",
    "                    if normalized_segment_for_dir_check in normalized_full_path:\n",
    "                        valid_files.append(full_path)\n",
    "                else: # Otherwise, add all found audio files\n",
    "                    valid_files.append(full_path)\n",
    "                \n",
    "    return valid_files\n",
    "\n",
    "# Updated input folder to point to the root of your synthetic datasets\n",
    "input_folder = './data/dataset_sintetico/'\n",
    "\n",
    "# --- Configuration for path filtering ---\n",
    "# Example 1: Filter for 'test' directory\n",
    "# required_segment_filter = os.sep + 'test' + os.sep \n",
    "# Example 2: Filter for paths containing 'test' as a substring\n",
    "required_segment_filter = '/test/'\n",
    "# Example 3: No path segment filtering\n",
    "# required_segment_filter = None\n",
    "# --- End Configuration ---\n",
    "\n",
    "file_paths = get_audio_files_recursive(\n",
    "    input_folder,\n",
    "    formats=('wav', 'flac'),\n",
    "    required_path_segment=required_segment_filter\n",
    ")\n",
    "\n",
    "# --- For debugging the paths (optional) ---\n",
    "# for p in file_paths[:5]: \n",
    "#     print(p)\n",
    "# print(f\"Total paths found before filtering (if any): {len(file_paths)}\")\n",
    "# print(\"---\")\n",
    "# --- End debugging ---\n",
    "\n",
    "# Determine file types found for the print message\n",
    "if file_paths:\n",
    "    found_formats = sorted(list(set(\n",
    "        [os.path.splitext(f)[1] for f in file_paths]\n",
    "    )))\n",
    "    formats_str = ', '.join(found_formats)\n",
    "else:\n",
    "    formats_str = 'N/A'\n",
    "\n",
    "# Updated print statement for better readability\n",
    "if required_segment_filter:\n",
    "    display_filter = (\n",
    "        required_segment_filter.strip(os.sep)\n",
    "        if os.sep in required_segment_filter\n",
    "        else required_segment_filter\n",
    "    )\n",
    "    print(\n",
    "        f\"Found {len(file_paths)} audio files with paths containing \"\n",
    "        f\"'{display_filter}' ({formats_str} formats)\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Found {len(file_paths)} audio files \"\n",
    "        f\"(no path segment filter applied) ({formats_str} formats)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and utility functions\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utility function to pad or truncate an audio signal to a fixed length.\n",
    "def pad(x, desired_length):\n",
    "    if len(x) >= desired_length:\n",
    "        return x[:desired_length]\n",
    "    else:\n",
    "        pad_width = desired_length - len(x)\n",
    "        return np.pad(x, (0, pad_width), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define a custom dataset to load WAV files\n",
    "\n",
    "class CustomWavDataset(Dataset):\n",
    "    def __init__(self, file_list, base_folder='./data/dataset_sintetico/', sr=16000, cut=64600):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_list (list): List of full paths to WAV files.\n",
    "            base_folder (str): Base folder to exclude from the relative path.\n",
    "            sr (int): Target sampling rate.\n",
    "            cut (int): Fixed length (in samples) for each audio waveform.\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.base_folder = os.path.normpath(base_folder)\n",
    "        self.sr = sr\n",
    "        self.cut = cut\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        # Load the audio file; force sample rate to 16 kHz.\n",
    "        signal, _ = librosa.load(file_path, sr=self.sr)\n",
    "        signal_padded = pad(signal, self.cut)\n",
    "        # Convert signal to a PyTorch tensor.\n",
    "        signal_tensor = torch.tensor(signal_padded, dtype=torch.float32)\n",
    "        \n",
    "        # Generate a relative path as the unique identifier\n",
    "        # First normalize the path to handle different path formats\n",
    "        norm_file_path = os.path.normpath(file_path)\n",
    "        norm_base_folder = os.path.normpath(self.base_folder)\n",
    "        \n",
    "        # Remove the base folder from the path to get the relative path\n",
    "        if norm_file_path.startswith(norm_base_folder):\n",
    "            # +1 to remove the leading slash\n",
    "            rel_path = norm_file_path[len(norm_base_folder) + 1:]\n",
    "        else:\n",
    "            # Fallback if the path doesn't start with the base folder\n",
    "            rel_path = os.path.basename(file_path)\n",
    "        \n",
    "        return signal_tensor, rel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4efaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Skip if using Cell 1\n",
    "# If Cell 1 was run, do not override file_paths\n",
    "if \"file_paths\" not in globals():\n",
    "    file_list_path = \"/root/rafaello/datasets/yourtts_wav_files.txt\"  # Update path if needed.\n",
    "    with open(file_list_path, \"r\") as f:\n",
    "        file_paths = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(file_paths)} wav file paths.\")\n",
    "else:\n",
    "    print(f\"Using dynamically detected {len(file_paths)} audio files from Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create the dataset and dataloader\n",
    "\n",
    "base_folder = input_folder\n",
    "dataset = CustomWavDataset(file_paths, base_folder=base_folder)\n",
    "# Adjust the batch size as needed.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES first\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Change if needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create minimal args with required parameters\n",
    "args = Namespace(\n",
    "    track='DF',\n",
    "    model_path='/app/SSL_Anti-spoofing/Best_LA_model_for_DF.pth',\n",
    "    protocols_path='/app/SSL_Anti-spoofing/protocols/',\n",
    "    database_path='/app/SSL_Anti-spoofing/database/',\n",
    "    loss='WCE'\n",
    ")\n",
    "\n",
    "from model import Model # This should still work due to the .pth file\n",
    "model = Model(args, device)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "checkpoint = torch.load(args.model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded in evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run inference with continuous save\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "load_scores_file = \"./outputs/df_detection_scores.pkl\"  # File to load existing scores\n",
    "save_scores_file = \"./outputs/df_detection_scores.pkl\"  # File to save scores\n",
    "force_recalculate = True        # Set to True to ignore saved scores and recompute\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(save_scores_file), exist_ok=True)\n",
    "\n",
    "# If not forcing recalculation and scores exist, load and skip inference\n",
    "if not force_recalculate and os.path.exists(load_scores_file):\n",
    "    print(f\"Loading existing scores from {load_scores_file}...\")\n",
    "    with open(load_scores_file, \"rb\") as f:\n",
    "        all_scores = pickle.load(f)\n",
    "    print(\"Scores loaded. Skipping inference.\")\n",
    "else:\n",
    "    print(\"No saved scores found or recalculation forced. Running inference...\")\n",
    "    all_scores = {}  # Initialize an empty dictionary\n",
    "\n",
    "    # Identify unprocessed files\n",
    "    all_utt_ids = set(file_paths)  # Assuming `file_paths` contains all expected files\n",
    "    processed_utt_ids = set(all_scores.keys())\n",
    "\n",
    "    # If all files are already processed, skip inference\n",
    "    if processed_utt_ids >= all_utt_ids:\n",
    "        print(f\"All {len(all_utt_ids)} files are already processed. Skipping inference.\")\n",
    "    else:\n",
    "        print(f\"Processing {len(all_utt_ids - processed_utt_ids)} new files...\")\n",
    "\n",
    "        # Track progress and save results\n",
    "        with torch.no_grad():\n",
    "            for batch_x, utt_ids in tqdm(dataloader, \n",
    "                                         desc=\"Processing batches\", \n",
    "                                         unit=\"batch\",\n",
    "                                         dynamic_ncols=True):\n",
    "                # Skip already processed files\n",
    "                utt_ids_to_process = [uid for uid in utt_ids if uid not in all_scores]\n",
    "                if not utt_ids_to_process:\n",
    "                    continue\n",
    "\n",
    "                # Process the batch\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_out = model(batch_x)\n",
    "                batch_scores = batch_out[:, 1].cpu().numpy()\n",
    "\n",
    "                # Update scores dictionary and save immediately\n",
    "                for utt_id, score in zip(utt_ids, batch_scores):\n",
    "                    all_scores[utt_id] = score\n",
    "\n",
    "                # Save scores after each batch\n",
    "                with open(save_scores_file, \"wb\") as f:\n",
    "                    pickle.dump(all_scores, f)\n",
    "\n",
    "        print(f\"\\nInference complete. Processed {len(all_scores)} files.\")\n",
    "        print(f\"Scores saved to: {save_scores_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize score distribution with threshold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Required for np.mean\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure scores are loaded\n",
    "if not 'all_scores' in globals():\n",
    "    if os.path.exists(scores_file):\n",
    "        print(f\"Loading scores from {scores_file}...\")\n",
    "        with open(scores_file, \"rb\") as f:\n",
    "            all_scores = pickle.load(f)\n",
    "    else:\n",
    "        raise RuntimeError(\"Scores file not found, and inference was not run.\")\n",
    "\n",
    "# Convert scores to a list\n",
    "scores = list(all_scores.values())\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(scores, bins=100, alpha=0.7, \n",
    "                            color='blue', edgecolor='black')\n",
    "\n",
    "# Add threshold line\n",
    "threshold = -3.5324\n",
    "plt.axvline(threshold, color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Threshold: {threshold:.4f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('DF Detection Scores - F5-TTS on SSL-Anti-Spoofing', fontsize=14)\n",
    "plt.xlabel('Score Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count annotations\n",
    "plt.text(0.05, 0.95, f'Total files: {len(scores)}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "plt.text(0.05, 0.90, f'Mean score: {np.mean(scores):.4f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
